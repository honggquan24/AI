{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9402352,"sourceType":"datasetVersion","datasetId":5707776}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nimport time","metadata":{"execution":{"iopub.status.busy":"2024-09-16T15:46:10.261891Z","iopub.execute_input":"2024-09-16T15:46:10.262604Z","iopub.status.idle":"2024-09-16T15:46:10.267135Z","shell.execute_reply.started":"2024-09-16T15:46:10.262567Z","shell.execute_reply":"2024-09-16T15:46:10.266120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/ex-123/* /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2024-09-16T15:46:10.270709Z","iopub.execute_input":"2024-09-16T15:46:10.271496Z","iopub.status.idle":"2024-09-16T15:46:11.313401Z","shell.execute_reply.started":"2024-09-16T15:46:10.271462Z","shell.execute_reply":"2024-09-16T15:46:11.312208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def function(*, X: np.ndarray, theta: np.ndarray, add_ones: True) -> np.ndarray:\n    \"\"\"\n    Predicts output values using the input features and model parameters.\n\n    Args:\n        X (np.ndarray): Input features.\n        theta (np.ndarray): Model parameters (theta).\n\n    Returns:\n        np.ndarray: Predicted output values.\n    \"\"\"\n    if add_ones:\n        ones = np.ones(len(X)).reshape(-1, 1)  # Add a column of ones for the intercept term\n        X_with_ones = np.hstack((ones, X))  # Combine input with ones\n    else: \n        X_with_ones = X\n    y_pred = np.sum(X_with_ones * theta.reshape(-1,), axis=1)  # Compute predictions\n        \n    return y_pred\n\ndef plot_graph(*, title_1: str, title_2: str, title_3: 0, style_1: str, style_2: str,  style_3: 0,\n               x1: np.ndarray, y1: np.ndarray, x2: np.ndarray, y2: np.ndarray,  x3: 0, y3: 0):\n    \"\"\"\n    Plots two sets of data points using Plotly.\n\n    Args:\n        title_1 (str): Name of the first plot line.\n        title_2 (str): Name of the second plot line.\n        style_1 (str): Plot style for the first line (e.g., 'markers').\n        style_2 (str): Plot style for the second line (e.g., 'lines').\n        x1, y1, x2, y2 (np.ndarray): Data points for both lines.\n    \"\"\"\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=x1, y=y1, mode=style_1, name=title_1))\n    fig.add_trace(go.Scatter(x=x2, y=y2, mode=style_2, name=title_2))\n    fig.add_trace(go.Scatter(x=x3, y=y3, mode=style_3, name=title_3))\n    fig.show()\n\ndef normalize_vector(vector: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Normalizes a vector to have zero mean and unit variance.\n\n    Args:\n        vector (np.ndarray): Input vector.\n\n    Returns:\n        np.ndarray: Normalized vector.\n    \"\"\"\n    mean = np.mean(vector)  # Calculate the mean\n    std = np.std(vector)  # Calculate the standard deviation\n    normalized_vector = (vector - mean) / std  # Normalize the vector\n    return normalized_vector\n\ndef create_polynomial_features(*, X: np.ndarray, order: int) -> np.ndarray:\n    \"\"\"\n    Converts an input vector into a matrix of polynomial features.\n\n    Args:\n        X (np.ndarray): Input feature vector.\n        degree (int): Degree of the polynomial.\n\n    Returns:\n        np.ndarray: Matrix of polynomial features.\n    \"\"\"\n    X = X.reshape(-1,)\n    poly_matrix = np.ones((len(X), order))  # Initialize the matrix with ones\n    for i in range(order):\n        poly_matrix[:, i] = X ** (i + 1)  # Raise each column to the respective power\n    return poly_matrix\n\ndef compute_true_theta(*, X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the true value of theta using the normal equation.\n\n    Args:\n        X (np.ndarray): Input feature matrix.\n        Y (np.ndarray): Output vector.\n\n    Returns:\n        np.ndarray: Computed theta values.\n    \"\"\"\n    ones = np.ones(len(X)).reshape(-1, 1)\n    X_with_ones = np.hstack((ones, X))  # Add a column of ones for the intercept term\n    inv_XT_X = np.linalg.inv(np.dot(X_with_ones.T, X_with_ones))  # Compute (X.T * X)^-1\n    XT_Y = np.dot(X_with_ones.T, Y.reshape(-1, 1))  # Compute (X.T * Y)\n    theta_true = np.dot(inv_XT_X, XT_Y)  # Compute theta\n    return theta_true\n\ndef denormalize_theta(*, theta_norm: np.ndarray, X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Converts normalized theta values back to the original scale.\n\n    Args:\n        theta_norm (np.ndarray): Normalized theta values.\n        X (np.ndarray): Input features.\n        Y (np.ndarray): Output values.\n\n    Returns:\n        np.ndarray: Denormalized theta values.\n    \"\"\"\n    theta = np.zeros_like(theta_norm)\n\n    mean_X = np.mean(X, axis=0)\n    std_X = np.std(X, axis=0)\n\n    mean_Y = np.mean(Y, axis=0)\n    std_Y = np.std(Y, axis=0)\n\n    theta[1:] = std_Y * theta_norm[1:] / std_X.reshape(-1, 1)\n    theta[0] = mean_Y + std_Y * theta_norm[0] - np.dot(std_Y * mean_X / std_X, theta_norm[1:])\n    return theta","metadata":{"execution":{"iopub.status.busy":"2024-09-16T15:46:11.315874Z","iopub.execute_input":"2024-09-16T15:46:11.316213Z","iopub.status.idle":"2024-09-16T15:46:11.334681Z","shell.execute_reply.started":"2024-09-16T15:46:11.316179Z","shell.execute_reply":"2024-09-16T15:46:11.333740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Linear_Regression_Multivariables:\n    def __init__(self, *, number_of_feature: int) -> None:\n        \"\"\"\n        Initializes the Linear Regression model.\n        Args:\n            num_features (int): Number of features in the input data.\n        \"\"\"\n        self.number_of_features = number_of_feature\n\n    def normalize_vector(self, vector: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Normalizes the input vector.\n        \"\"\"\n        mean = np.mean(vector)\n        std = np.std(vector)\n        return (vector - mean) / std\n    \n    def normalize_input_output(self, *, X: np.ndarray, Y: np.ndarray) -> tuple:\n        \"\"\"\n        Normalizes the input features and output values.\n\n        Args:\n            X (np.ndarray): Input features.\n            Y (np.ndarray): Output values.\n\n        Returns:\n            tuple: Normalized input features and output values.\n        \"\"\"\n        norm_X = np.apply_along_axis(self.normalize_vector, arr=X, axis=0).reshape(-1, self.number_of_features)\n        norm_Y = np.apply_along_axis(self.normalize_vector, arr=Y, axis=0).reshape(-1, 1)\n        return norm_X, norm_Y\n\n    def add_ones_columns(self, *, normalized_input: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Adds a column of ones to the input features for the intercept term.\n\n        Args:\n            X (np.ndarray): Normalized input features.\n\n        Returns:\n            np.ndarray: Input features with an added column of ones.\n        \"\"\"\n        ones = np.ones(len(normalized_input)).reshape(-1, 1)\n        x_add = np.hstack((ones, normalized_input))\n        return x_add\n\n    def predict(self, *, theta: np.ndarray, normalized_input: np.ndarray) -> np.ndarray:\n        y_pred = np.matmul(normalized_input, theta)\n        return y_pred\n    \n    def compute_loss(self, *, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n        m = len(y_true)\n        E = y_pred - y_true\n        J = np.sum((E)**2) / (2*m)\n        return J\n    \n    def update_params(self, *, theta: np.ndarray, lr: float, y_pred: np.ndarray, \n                      y_true: np.ndarray, normalized_input: np.ndarray) -> np.ndarray:\n        m = len(y_true)\n        E = y_pred - y_true\n        dJ_dtheta = np.dot(normalized_input.T, E) / (m)\n        theta_updated = theta - lr*dJ_dtheta\n        return theta_updated\n    \n    def denormalize_theta(self, *, theta_normalized: np.ndarray, input: np.ndarray, output: np.ndarray) -> np.ndarray:\n        theta = np.zeros_like(theta_normalized)\n\n        mean_x = np.mean(input,  axis= 0)\n        std_x = np.std(input,  axis= 0)\n\n        mean_y = np.mean(output, axis= 0)\n        std_y = np.std(output, axis= 0)\n\n        theta[1:] = std_y*theta_normalized[1:]/(std_x.reshape(-1, 1))\n        theta[0] = mean_y + std_y*theta_normalized[0] - np.dot(std_y*mean_x/std_x, theta_normalized[1:])\n\n        return theta\n    \n    def plot_graph(self, data):\n        clear_output(wait= True)\n        fig = go.FigureWidget()\n        for i in range(len(data)):\n            fig.add_trace(go.Scatter(x=data[i]['x'], y=data[i]['y'],\n                                     mode= data[i]['mode'], name= data[i]['title']))\n        fig.show()\n        time.sleep(1/2)\n    \n    def train(self, *, epoch: int, theta: np.ndarray, input: np.ndarray, \n              output: np.ndarray, lr: float, plot_graph: False) -> np.ndarray:\n        \"\"\"\n        Trains the Linear Regression model using gradient descent.\n\n        Args:\n            epochs (int): Number of training iterations.\n            theta (np.ndarray): Initial model parameters.\n            X (np.ndarray): Input features.\n            Y (np.ndarray): Output values.\n            lr (float): Learning rate for parameter updates.\n\n        Returns:\n            tuple: Array of loss values and the trained model parameters.\n        \"\"\"\n        normalized_input, normalized_ouput = self.normalize_input_output(X= input, Y= output)\n        normalized_input_with_ones = self.add_ones_columns(normalized_input= normalized_input)\n\n        J_array = np.array([])\n        for i in range(epoch):\n            y_pred = self.predict(theta= theta, normalized_input= normalized_input_with_ones)\n            J = self.compute_loss(y_true= normalized_ouput, y_pred= y_pred)\n            theta = self.update_params(theta= theta, lr= lr, y_pred= y_pred, \n                                       y_true= normalized_ouput, normalized_input= normalized_input_with_ones)\n            J_array = np.append(arr= J_array, values= J)\n            \n            if plot_graph == True:\n                x_with_ones = self.add_ones_columns(normalized_input= input)\n                theta_praph = self.denormalize_theta(theta_normalized= theta, input= input, output= output)\n                data_1 = dict(x= input[:, 0].reshape(-1,), y= output.reshape(-1,), mode= 'markers', title= 'Data')\n                data_2 = dict(x= input[:, 0].reshape(-1,), y= function(X= x_with_ones, theta= theta_praph, add_ones= False) , mode= 'lines', title= 'Predicted')\n                data = [data_1, data_2]\n                self.plot_graph(data= data)\n        \n        theta = self.denormalize_theta(theta_normalized= theta, input= input, output= output)\n        return J_array, theta","metadata":{"execution":{"iopub.status.busy":"2024-09-16T15:46:11.336086Z","iopub.execute_input":"2024-09-16T15:46:11.336388Z","iopub.status.idle":"2024-09-16T15:46:11.360264Z","shell.execute_reply.started":"2024-09-16T15:46:11.336342Z","shell.execute_reply":"2024-09-16T15:46:11.359416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ex2","metadata":{}},{"cell_type":"code","source":"# Read csv file ex2.csv\npd_ex2 = pd.read_csv('ex2.csv')\n\n# Get collumns of file \nX_cols = pd_ex2.columns[:-1]\nY_col = pd_ex2.columns[-1]","metadata":{"execution":{"iopub.status.busy":"2024-09-16T15:46:11.362569Z","iopub.execute_input":"2024-09-16T15:46:11.363015Z","iopub.status.idle":"2024-09-16T15:46:11.406094Z","shell.execute_reply.started":"2024-09-16T15:46:11.362973Z","shell.execute_reply":"2024-09-16T15:46:11.405337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get vector input and output\nX = pd_ex2[X_cols].values\nY = pd_ex2[Y_col].values","metadata":{"execution":{"iopub.status.busy":"2024-09-16T15:46:11.407122Z","iopub.execute_input":"2024-09-16T15:46:11.407391Z","iopub.status.idle":"2024-09-16T15:46:11.421063Z","shell.execute_reply.started":"2024-09-16T15:46:11.407361Z","shell.execute_reply":"2024-09-16T15:46:11.420142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(1)\n\ntheta_init = np.random.randn(len(X_cols) + 1, 1)\ntheta_init.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-16T15:46:11.422517Z","iopub.execute_input":"2024-09-16T15:46:11.423348Z","iopub.status.idle":"2024-09-16T15:46:11.432676Z","shell.execute_reply.started":"2024-09-16T15:46:11.423300Z","shell.execute_reply":"2024-09-16T15:46:11.431844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Linear_Regression_Multivariables(number_of_feature= 8)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T15:47:00.684124Z","iopub.execute_input":"2024-09-16T15:47:00.684965Z","iopub.status.idle":"2024-09-16T15:47:00.689611Z","shell.execute_reply.started":"2024-09-16T15:47:00.684917Z","shell.execute_reply":"2024-09-16T15:47:00.688605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate_ = [0.01, 0.001, 0.003, 0.3, 0.04, 0.1]\nJ = np.zeros((1000, ))\nfor i in learning_rate_:\n    \n    J_arr, theta_arr = model.train(epoch= 1000, theta= theta_init, \n                                           input= X, output= Y, lr= i, plot_graph= False )\n    \n    J = np.vstack([J, J_arr])\n\nfig = go.Figure()\nfor i in range(len(J[1:, :])):\n    fig.add_trace(go.Scatter(x=np.arange(1000), y=J[(1+i), :],\n                            mode= 'lines', name= f'lr: {learning_rate_[i]}'))\nfig.update_xaxes(title= 'epochs')\nfig.update_yaxes(title= 'J', tickangle= 0)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T15:47:01.115035Z","iopub.execute_input":"2024-09-16T15:47:01.115368Z","iopub.status.idle":"2024-09-16T15:47:03.165468Z","shell.execute_reply.started":"2024-09-16T15:47:01.115333Z","shell.execute_reply":"2024-09-16T15:47:03.164571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Linear_Regression_Multivariables(number_of_feature= 8)\nJ_array_ex_2, theta_ex_2 = model.train(\n    epoch= 1000, theta= theta_init, input= X, output= Y, lr= 0.3, plot_graph= False\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T15:47:03.167321Z","iopub.execute_input":"2024-09-16T15:47:03.167965Z","iopub.status.idle":"2024-09-16T15:47:03.518371Z","shell.execute_reply.started":"2024-09-16T15:47:03.167922Z","shell.execute_reply":"2024-09-16T15:47:03.517109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"theta_real = compute_true_theta(X= X, Y= Y)\nX_with_ones = model.add_ones_columns(normalized_input= X)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T15:47:03.520042Z","iopub.execute_input":"2024-09-16T15:47:03.520735Z","iopub.status.idle":"2024-09-16T15:47:03.536145Z","shell.execute_reply.started":"2024-09-16T15:47:03.520673Z","shell.execute_reply":"2024-09-16T15:47:03.534801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"theta_difference = np.round((theta_real - theta_ex_2) / theta_real, 2)\ntheta_difference","metadata":{"execution":{"iopub.status.busy":"2024-09-16T15:47:03.539269Z","iopub.execute_input":"2024-09-16T15:47:03.540056Z","iopub.status.idle":"2024-09-16T15:47:03.549018Z","shell.execute_reply.started":"2024-09-16T15:47:03.539993Z","shell.execute_reply":"2024-09-16T15:47:03.547845Z"},"trusted":true},"execution_count":null,"outputs":[]}]}